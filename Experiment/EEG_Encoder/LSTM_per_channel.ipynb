{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from torchinfo import summary\n",
    "\n",
    "class classifier_LSTM_per_channel(nn.Module):\n",
    "    def __init__(self, channels_num):\n",
    "        super(classifier_LSTM_per_channel, self).__init__()\n",
    "        self.lstm_size0 = 1\n",
    "        self.channels_num = channels_num\n",
    "        self.lstm_per_channel = nn.LSTM(1, hidden_size=self.lstm_size0, batch_first=True)\n",
    "        self.lstm_full1 = nn.LSTM(32, hidden_size=128, batch_first=True)\n",
    "        self.lstm_full2 = nn.LSTM(128, hidden_size=64, batch_first=True)\n",
    "        self.linear = nn.Linear(64, out_features=6)\n",
    "        self.softmax = nn.Softmax()\n",
    "    def forward(self, X):\n",
    "        #X: (batch_size, channels_num, sequence_len)\n",
    "        print(X.size())\n",
    "        batch_size = len(X)\n",
    "        lstm_init = (torch.zeros(1, batch_size, self.lstm_size0),\n",
    "                     torch.zeros(1, batch_size, self.lstm_size0))\n",
    "        # if x.is_cuda:\n",
    "        #     lstm_init = (lstm_init[0].cuda(self.GPUindex),\n",
    "        #                  lstm_init[0].cuda(self.GPUindex))\n",
    "        lstm_init = (Variable(lstm_init[0]), Variable(lstm_init[1]))\n",
    "        first_layer_inputs = []\n",
    "        second_layer_inputs = [] #(32, timesteps)\n",
    "        for i in range(self.channels_num): \n",
    "            x_in = torch.tensor(X[:, i, :]) # x_in: (batch_size, 1, timesteps)\n",
    "            x_in = torch.squeeze(x_in, 1) #x_in: (batch_size, timesteps)\n",
    "            x_in = torch.unsqueeze(x_in, -1) #x_in: (batch_size, timesteps, input_size=1)\n",
    "            first_layer_inputs.append(x_in)\n",
    "            x_out = self.lstm_per_channel(x_in, lstm_init)[0] # x_out: (batch_size, timesteps, 1)\n",
    "            second_layer_inputs.append(x_out)\n",
    "\n",
    "        X = torch.cat(second_layer_inputs, -1) #X: (batch_size, timesteps, 32)\n",
    "        print(\"X after concat: \", X.size())\n",
    "        X = self.lstm_full1(X)[0] # output, (hx, cx)\n",
    "        X = self.lstm_full2(X)[0]\n",
    "        X = self.linear(X)\n",
    "        X = self.softmax(X)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 32, 15360])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_18876\\728937785.py:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_in = torch.tensor(X[:, i, :]) # x_in: (batch_size, 1, timesteps)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X after concat:  torch.Size([10, 15360, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_18876\\728937785.py:43: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  X = self.softmax(X)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "classifier_LSTM_per_channel              [10, 15360, 6]            --\n",
       "├─LSTM: 1-1                              [10, 15360, 1]            16\n",
       "├─LSTM: 1-2                              [10, 15360, 1]            (recursive)\n",
       "├─LSTM: 1-3                              [10, 15360, 1]            (recursive)\n",
       "├─LSTM: 1-4                              [10, 15360, 1]            (recursive)\n",
       "├─LSTM: 1-5                              [10, 15360, 1]            (recursive)\n",
       "├─LSTM: 1-6                              [10, 15360, 1]            (recursive)\n",
       "├─LSTM: 1-7                              [10, 15360, 1]            (recursive)\n",
       "├─LSTM: 1-8                              [10, 15360, 1]            (recursive)\n",
       "├─LSTM: 1-9                              [10, 15360, 1]            (recursive)\n",
       "├─LSTM: 1-10                             [10, 15360, 1]            (recursive)\n",
       "├─LSTM: 1-11                             [10, 15360, 1]            (recursive)\n",
       "├─LSTM: 1-12                             [10, 15360, 1]            (recursive)\n",
       "├─LSTM: 1-13                             [10, 15360, 1]            (recursive)\n",
       "├─LSTM: 1-14                             [10, 15360, 1]            (recursive)\n",
       "├─LSTM: 1-15                             [10, 15360, 1]            (recursive)\n",
       "├─LSTM: 1-16                             [10, 15360, 1]            (recursive)\n",
       "├─LSTM: 1-17                             [10, 15360, 1]            (recursive)\n",
       "├─LSTM: 1-18                             [10, 15360, 1]            (recursive)\n",
       "├─LSTM: 1-19                             [10, 15360, 1]            (recursive)\n",
       "├─LSTM: 1-20                             [10, 15360, 1]            (recursive)\n",
       "├─LSTM: 1-21                             [10, 15360, 1]            (recursive)\n",
       "├─LSTM: 1-22                             [10, 15360, 1]            (recursive)\n",
       "├─LSTM: 1-23                             [10, 15360, 1]            (recursive)\n",
       "├─LSTM: 1-24                             [10, 15360, 1]            (recursive)\n",
       "├─LSTM: 1-25                             [10, 15360, 1]            (recursive)\n",
       "├─LSTM: 1-26                             [10, 15360, 1]            (recursive)\n",
       "├─LSTM: 1-27                             [10, 15360, 1]            (recursive)\n",
       "├─LSTM: 1-28                             [10, 15360, 1]            (recursive)\n",
       "├─LSTM: 1-29                             [10, 15360, 1]            (recursive)\n",
       "├─LSTM: 1-30                             [10, 15360, 1]            (recursive)\n",
       "├─LSTM: 1-31                             [10, 15360, 1]            (recursive)\n",
       "├─LSTM: 1-32                             [10, 15360, 1]            (recursive)\n",
       "├─LSTM: 1-33                             [10, 15360, 128]          82,944\n",
       "├─LSTM: 1-34                             [10, 15360, 64]           49,664\n",
       "├─Linear: 1-35                           [10, 15360, 6]            390\n",
       "├─Softmax: 1-36                          [10, 15360, 6]            --\n",
       "==========================================================================================\n",
       "Total params: 133,014\n",
       "Trainable params: 133,014\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 20.45\n",
       "==========================================================================================\n",
       "Input size (MB): 19.66\n",
       "Forward/backward pass size (MB): 244.53\n",
       "Params size (MB): 0.53\n",
       "Estimated Total Size (MB): 264.72\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = classifier_LSTM_per_channel(channels_num=32)\n",
    "summary(model, input_size=(10, 32, 15360)) #(batch_size, channels_num, timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  5 2022, 06:56:58) \n[GCC 7.5.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "b90999d3235f3a774a580063db71b4b169c822c69c44dbe8be761b1b7628ba5d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
